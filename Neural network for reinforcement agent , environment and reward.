import gym
import numpy as np
import tensorflow as tf

# Define the neural network model
class PolicyNetwork(tf.keras.Model):
    def __init__(self, num_actions):
        super(PolicyNetwork, self).__init__()
        self.dense1 = tf.keras.layers.Dense(128, activation='relu')
        self.dense2 = tf.keras.layers.Dense(num_actions, activation='softmax')

    def call(self, state):
        x = self.dense1(state)
        return self.dense2(x)

# Define the reinforcement learning agent
class ReinforcementAgent:
    def __init__(self, num_actions):
        self.policy_network = PolicyNetwork(num_actions)
        self.optimizer = tf.keras.optimizers.Adam()

    def choose_action(self, state):
        policy = self.policy_network(np.array([state]))
        action = np.random.choice(len(policy[0]), p=policy.numpy()[0])
        return action

    def train(self, states, actions, rewards):
        with tf.GradientTape() as tape:
            # Calculate the log probability of the chosen action
            probs = self.policy_network(states)
            action_masks = tf.one_hot(actions, len(probs[0]))
            log_probs = tf.math.log(tf.reduce_sum(probs * action_masks, axis=1))
            
            # Calculate the loss (negative log probability multiplied by rewards)
            loss = -tf.reduce_mean(log_probs * rewards)

        # Update the model weights
        gradients = tape.gradient(loss, self.policy_network.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.policy_network.trainable_variables))

# Define the environment
env = gym.make('CartPole-v1')

# Initialize the agent
agent = ReinforcementAgent(env.action_space.n)

# Training loop
for episode in range(1000):
    state = env.reset()
    total_reward = 0
    states, actions, rewards = [], [], []
    done = False

    while not done:
        # Choose action based on current state
        action = agent.choose_action(state)

        # Take action and observe next state and reward
        next_state, reward, done, _ = env.step(action)

        # Store state, action, and reward for training
        states.append(state)
        actions.append(action)
        rewards.append(reward)

        state = next_state
        total_reward += reward

    # Train the agent
    agent.train(np.array(states), np.array(actions), np.array(rewards))

    print(f"Episode: {episode + 1}, Total Reward: {total_reward}")

env.close()
